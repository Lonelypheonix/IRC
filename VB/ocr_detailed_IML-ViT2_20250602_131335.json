{
  "timestamp": "2025-06-02T13:13:36.013974",
  "pdf_file": "IML-ViT2.pdf",
  "total_pages": 16,
  "pages": [
    {
      "page_number": 1,
      "markdown": "# IML-ViT: Benchmarking Image Manipulation Localization by Vision Transformer \n\nXiaochen Ma ${ }^{1}$, Bo Du ${ }^{1}$, Zhuohang Jiang ${ }^{1}$, Xia Du ${ }^{2}$, Ahmed Y. Al Hammadi ${ }^{3}$, and Jizhe Zhou ${ }^{1}$<br>${ }^{1}$ Sichuan University<br>${ }^{2}$ Xiamen University of Technology<br>${ }^{3}$ Mohamed Bin Zayed University for Humanities\n\n\n#### Abstract\n\nAdvanced image tampering techniques are increasingly challenging the trustworthiness of multimedia, leading to the development of Image Manipulation Localization (IML). But what makes a good IML model? The answer lies in the way to capture artifacts. Exploiting artifacts requires the model to extract non-semantic discrepancies between manipulated and authentic regions, necessitating explicit comparisons between the two areas. With the self-attention mechanism, naturally, the Transformer should be a better candidate to capture artifacts. However, due to limited datasets, there is currently no pure ViT-based approach for IML to serve as a benchmark, and CNNs dominate the entire task. Nevertheless, CNNs suffer from weak long-range and non-semantic modeling. To bridge this gap, based on the fact that artifacts are sensitive to image resolution, amplified under multi-scale features, and massive at the manipulation border, we formulate the answer to the former question as building a ViT with high-resolution capacity, multi-scale feature extraction capability, and manipulation edge supervision that could converge with a small amount of data. We term this simple but effective ViT paradigm IML-ViT, which has significant potential to become a new benchmark for IML. Extensive experiments on three different mainstream protocols verified our model outperforms the state-of-the-art manipulation localization methods. Code and models are available at https://github.com/SunnyHaze/IML-ViT.\n\n\n## 1. Introduction\n\nWith the advances in image editing technology like Photoshop, Image Manipulation Localization (IML) methods have become urgent countermeasures to cope with existing tampered images and avoid security threats [33]. Effective IML methods play a crucial role in discerning misinformation and have the potential to contribute to the safety of multimedia world. As shown in Figure 1, the IML task aims to detect whether images have been modified and to localize the modified regions at the pixel level. Image manipulation can be\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1. An example of three types of manipulations and their corresponding artifacts. Artifacts contain visible traces, including distortions, sudden changes, or anomalies caused by tampering operations. Artifacts are frequently found at the junction between two regions and appear in very detailed positions. For a better view, zooming in is recommended.\ngenerally classified into three types [33, 36]: (1) splicing: copying a region from an image and pasting it to another image. (2) copy-move: cloning a region within an image. (3) inpainting: erasing regions from images and inpaint missing regions with visually plausible contents.\n\nAs shown in Table 1, most existing methods for IML tasks greatly benefit from tracing artifacts with various CNNbased feature extractors. \"Artifacts\" refer to unique visible traces (see Figure 1) and invisible low-level feature inconsistencies (e.g., noise or high-frequency) resulting from manipulation. As tampering aims to deceive the audience by creating semantically meaningful and perceptually convincing images, visual traces typically manifest at a non-semantic level, distributed in textures around the manipulated area. Additionally, low-level features, like noise inconsistencies introduced by different cameras, can also serve as crucial evidence to reveal manipulated regions within the authentic area. Thus, based on previous experiences, the key to IML lies in capturing the artifacts by identifying non-semantic visible traces and low-level inconsistencies.\n\nHowever, convolution propagates information in a collective manner, making CNNs more suitable for semantic-\n\n[Figure 1] Figure 1. An example of three types of manipulations and their\ncorresponding artifacts. Artifacts contain visible traces, including\ndistortions, sudden changes, or anomalies caused by tampering\noperations. Artifacts are frequently found at the junction between\ntwo regions and appear in very detailed positions. For a better view,\nzooming in is recommended.  This figure is an output example of a model's performance on various tasks, likely related to image segmentation or object detection. It includes four main sections, each showing a different scene: cupcakes, a baseball player, a car, and a ball on a table. Each scene is accompanied by a silhouette of the object of interest (e.g., a baseball player, a car, or a ball) and a bounding box around the object. The bounding boxes are highlighted with red dashed lines, indicating the model's predictions. The figure demonstrates the model's ability to identify and segment objects in diverse contexts. The text \"Copy move\" and \"Inpainting\" on the left side suggests that the model is being evaluated on tasks involving copying and inpainting. The red and black bars on the right side of each scene might represent error metrics or confidence scores for the model's predictions.\n",
      "has_image": false
    },
    {
      "page_number": 2,
      "markdown": "| Method | Backbone | | Resolution | Manipulation supervision | IML dataset thirsty | |\n| --- | --- | --- | --- | --- | --- | --- |\n| | CNN | Tran. | | | Type | Amount |\n| MaxTau Net | $\\checkmark$ | - | Resize | Noise | Private | 102k |\n| [30] | | | 512×512 | (BayarConv+ SRM filter) | | |\n| SPAN | $\\checkmark$ | - | Resize | Noise | Private | 102k |\n| [15] | | | 224×224 | (BayarConv+ SRM filter) | | |\n| CRA-Net | $\\checkmark$ | - | Resize | Noise | Public | 5k |\n| [30] | | | short side to 800 | (BayarConv+ SRM filter) | | |\n| GSR-Net | $\\checkmark$ | - | Resize | Edge Prediction | Public | 5k |\n| [41] | | | 380×300 | (BayarConv) | | |\n| MVSS-Net | $\\checkmark$ | - | Resize | Noise (BayarConv) | Public | 5k |\n| [2] | | | 512×512 | Edge (solid) | | |\n| MM-Net | $\\checkmark$ | - | Resize | Noise (BayarConv) | Private | 50k |\n| [30] | | | short side to 800 | (BayarConv) | | |\n| TransForensic | $\\checkmark$ | $\\checkmark$ | Resize | - | Public | 10k |\n| [13] | | | 512×512 | - | | |\n| ObjectFormer | $\\checkmark$ | $\\checkmark$ | Resize | High-frequency | Private | 62K |\n| [15] | | | 236×236 | | | |\n| HiFi-Net | $\\checkmark$ | - | Resize | Frequency | Public | 1,710K |\n| [15] | | | 236×236 | | | |\n| TruFor | $\\checkmark$ | $\\checkmark$ | Crop | Noise | Public | 35K |\n| [13] | | | 512×512 | (Contrastive learning) | | |\n| IML-ViT | - | $\\checkmark$ | Zero-pad | Edge loss | Public | 5k |\n| (Thera) | | | 1024×1024 | | | |\n\nTable 1. Overview of State-of-the-Art End-to-End Models for Image Manipulation Localization. Tran. stands for Transformer. Manipulation supervision serves as prior knowledge broadly acknowledged in the image manipulation detection field. Edge information effectively traces visible artifacts, while noise and highfrequency features primarily highlight low-level differences between tampered and authentic regions.\nrelated tasks, such as object detection, rather than tracing non-semantic artifacts that often surround an object. Further, to identify low-level inconsistencies, we need to explicitly compare the relationships between different regions. But in deeper networks, CNNs may overlook global dependencies [26], rendering them less effective in capturing differences between regions. Given the weaknesses of CNN in non-semantic and long-distance modeling, we ask: Is there any other optimal backbone for solving IML tasks?\n\nConsidering the goal of capturing the feature discrepancies between the manipulated and authentic regions, we argue that self-attention should be a better solution regarding IML. As self-attention can explicitly model relationships between any areas regardless of their visual semantic relevance, especially for non-adjacent regions. The performance boost achieved by SPAN [15] highlights the effectiveness of integrating self-attention structures into convolutional layers. Furthermore, as artifacts are often distributed at the patch level rather than at the pixel or image level, Vision Transformer (ViT) [8] naturally becomes the ideal choice to trace artifacts and make comparisons.\n\nWhile ViT may be suitable for IML tasks, directly applying the original ViT architecture is insufficient. We suggest that IML involves three key discrepancies from traditional segmentation tasks, which also have not yet received sufficient attention in previous IML methods, as supported by Table 1. These discrepancies are:\n\nHigh Resolution While semantic segmentation and IML share similar inputs and outputs, IML tasks are more information-intensive, focusing on detailed artifacts rather than macro-semantics at the object level. Existing methods\nuse various extractors to trace artifacts, but their resizing methods already harm these first-hand artifacts. Therefore, preserving the original resolution of the images is crucial to retain essential artifacts for the model to learn.\n\nEdge Supervision As mentioned earlier, IML's primary focus lies in detecting the distinction between the tampered and authentic regions. This distinction is most pronounced at the boundary of the tampered region, whereas typical semantic segmentation tasks only require identifying information within the target region. From another perspective, it becomes evident that visible artifacts are more concentrated along the periphery of the tampered region rather than within it (as shown in Figure 1). Consequently, the IML task must guide the model to concentrate on the manipulated region's edges and learn its distribution for better performance.\n\nMulti-scale Supervision The percentage of tampered area to the total area varies significantly across different IML datasets. CASIAv2 [7] contains a considerable amount of sky replacement tampering, whereas Defacto [27] mostly consists of small object manipulations. On average, CASIAv2 has $7.6 \\%$ of pixels as tampered areas, while Defacto has only $1.7 \\%$. Additionally, IML datasets are labor-intensive and often limited in size, which poses challenges in bridging the gap between datasets. Therefore, incorporating multiscale supervision from the pre-processing and model design stages is essential to enhance generalization across different datasets.\n\nIn this paper, we present IML-ViT, an end-to-end ViTbased model that solves IML tasks. Regarding the proposed three key discrepancies, we devise IML-ViT with the following components: 1) a windowed ViT which accepts high-resolution input. Most of the global attention block is replaced with windowed attention as the trade-off for time complexity. We initialize it with Masked Autoencoder (MAE) [14] pre-trained parameters on ImageNet-1k [5]; 2) a simple feature pyramid networt (SFPN) [20] to introduce multi-scale supervision; 3) a morphology-based edge loss strategy is proposed to ensure edge supervision. The overview of IML-ViT is shown in Figure 2.\n\nIn this manner, without any specialized modules, IMLViT offers a general ViT structure for IML tasks. In other words, IML-ViT proves that IML tasks can be solved without hand-crafted features or deliberate feature fusion process, promoting future IML methods into a more generalizable design paradigm.\n\nTo the best of our knowledge, ObjectFormer [34], TransForensics [13], and TruFor [11] are the only Transformerrelated models solving the IML tasks. However, their backbone distinguishes significantly from vanilla ViT, as will be explained in Section 2. Thus, IML-ViT can be regarded as the pioneering model utilizing a vanilla ViT as the backbone for IML tasks.\n\nCurrently, the evaluation protocol for IML tasks is rather",
      "has_image": false
    },
    {
      "page_number": 3,
      "markdown": "![img-1.jpeg](img-1.jpeg)\n\nFigure 2. Overview of the general structure of IML-ViT.\n\nchaotic. To bring faithful evaluations and establish IML-ViT as the benchmark model, we demarcate existing messy evaluation settings into three mainstream protocols and conduct comprehensive experiments across these protocols. The extensive experiment results demonstrate that IML-ViT has surpassed all SoTA (state-of-the-art) models, thereby validating the reliability of the three proposed key essences of IML. Thus, we believe that IML-ViT is a powerful candidate to become a new SoTA model for IML.\n\nIn summary, our contributions are as follows:\n\n- We reveal the significant discrepancies between IML and traditional segmentation tasks by raising the three essences, which were overlooked by previous studies: high resolution, multi-scale, and edge supervision.\n- Aiming at three essences, we modify the components of ViT and establish the IML-ViT, the first ViT-based model for image manipulation localization.\n- Extensive experiments show that IML-ViT outperforms state-of-the-art models in both F1 and AUC scores on various protocols. This verifies the solidity of the three essences we proposed.\n- We vanish the evaluation barrier for future studies by demarcating existing evaluation settings into three mainstream protocols and implementing cross-protocols-comparisons.\n\n## 2. Related Works\n\n**Paradigm of IML** Research in the early years focused on a single kind of manipulation detection, with studies on copy-move [4, 30], splicing [3, 16, 17], and removal (Inpainting) [45], respectively. However, since the specific type of tampering is unknown in practice, after 2018, general manipulation detection has become the focus. Many existing works follow the paradigm of \"feature extraction + backbone inference\", especially extractors to exploit tamper-related information from artifacts. CR-CNN [38] has a noise-sensitive BayarConv [1] as the first convolution layer. RGB-N networks [43] develop an SRM filter to mine the difference in noise distribution to support decision-making. ManTra-Net [36] and SPAN [15] combined SRM, BayarConv, and as the first layer of their model. Besides noise-related extractors, ObjectFormer [34] employs a DCT module to extract high-frequency features, which are then combined with RGB features and fed into a transformer decoder. And MVSS-Net [2] combines a Sobel-supervised edge branch and a BayarConv noise branch with dual attention to fuse them. Nevertheless, a feature may only be effective for a single type of tampering, e.g., noise is more sensitive to splicing from different images but less effective for copy-move from the same image. Recently, TruFor [11] and NCL [41] are the first to explore utilizing contrastive learning to extract features instead of manually designed filters. Proposed IML-ViT also aims to step out of the paradigm of \"extraction + fusion\" and let the model itself learn as much knowledge as possible from the datasets rather than rely on priori knowledge.\n\n**Transformer-based IML method** At present, there are three Transformer-based models in the field of IML, namely ObjectFormer [34], TransForensics [13], and TruFor [11]. Though named \"Trans\" or \"Former\", these models are hardly in line with vanilla ViT in overall structures and design philosophies. In particular, different from ViT directly embedding the patched images before encoding, the first two methods utilize several CNN layers to extract feature maps initially and subsequently employ Transformers for further encoding, leading to neglecting crucial first-hand low-level information. On the other hand, TruFor follows SegFormer [37]'s encoder, using convolution layers instead of position embedding to integrate the position information for Transformer blocks, which overlooked key global dependencies to capture differences between regions.\n\nMoreover, in ObjectFormer's encoder, the \"query\" inputs are learnable vectors representing object prototypes o_i, not image embeddings. As a result, it focuses on cap-\n\n[Figure 2] Figure 2. Overview of the general structure of IML-ViT.  This figure is a model architecture diagram, illustrating the structure of a machine learning model used for image segmentation or object detection. Key visual elements include:\n\n- **Input Image**: Represented as a 3-channel image (H x W x 3) with padding.\n- **Patch Embedding**: The image is divided into patches, with padding applied to ensure uniformity.\n- **Windowed ViT-B Backbone**: This section shows the application of ViT-B architecture with global attention blocks, resulting in feature maps of dimensions H/16 x W/16 x 768.\n- **Multi-scale Supervision**: This part includes multi-scale feature maps (H/8 x W/8 x 256 and H/16 x W/16 x 256) for supervision.\n- **SFPN**: The Spatial Feature Pyramid Network processes the feature maps.\n- **Prediction and Loss Functions**: The model predicts the output and calculates the loss, with a focus on edge loss and prediction loss.\n- **Edge Mask**: The figure also shows the edge mask of the input image, used for edge detection and loss calculation.\n\nThe figure mainly demonstrates the flow of data through the model, from input image to prediction and loss calculation, emphasizing the use of multi-scale supervision and edge loss for improved performance. Notable elements include the dimensions of the feature maps and the use of padding for uniform patch division.\n",
      "has_image": false
    },
    {
      "page_number": 4,
      "markdown": "turing dependencies between object prototypes and image tokens, whereas a standard ViT encoder solely models the relationship between image embeddings. Besides, ObjectFormer is pre-trained with a large tampering-oriented synthesized private dataset, while IML-ViT achieves better performance with pre-training on the more accessible ImageNet-1k dataset.\n\nFurther, TransForensics has a different way to apply Transformer blocks. While ViT uses these blocks sequentially, TransForensics employs them in parallel, wherein each feature map of an FCN output is decoded with a Transformer block, and then fused for the final output.\n\nIn short, IML-ViT can be considered the first IML method with a vanilla ViT as its backbone and could easily benefit from recently advanced algorithms related to ViT, proving that IML tasks do not require complex designs.\n\n## 3 Proposed Method\n\nIn this section, we introduce our powerful IML-ViT paradigm, as shown in Figure 2, it consists of three main components: (1) a windowed ViT to balance the high-resolution inputs and the space complexity; (2) a simple feature pyramid network (SFPN) to introduce multi-scale features; and (3) a lightweight MLP decoder head with additional edge supervision, which aids in focusing on artifact-related features and ensures stable convergence.\n\n### 3.1 ViT Backbone\n\nHigh Resolution The ViT Encoder aims to mine the detailed artifacts and explore the differences between the suspicious areas. Thus, it is essential to preserve the original resolution of each image to avoid downsampling that could potentially distort the artifacts. However, when training in parallel, all images within a batch must have the same resolution. To reconcile these demands, we adopt a novel approach that has not been applied to any IML method before. Rather than simply rescaling images to the same size, we pad images and ground truth masks with zeros and place the image on the top-left side to match a larger constant resolution. This strategy maintains crucial low-level visual information of each image, allowing the model to explore better features instead of depending on handcrafted prior knowledge. To implement this approach, we first adjust the embedding dimensions of the ViT encoder to a larger scale.\n\nWindowed Attention To balance the computation cost from high resolution, we adopt a technique from previous works [20, 21], which periodically replaces part of the global attention blocks in ViT with windowed attention blocks. This method ensures global information propagation while reducing complexity. Differing from Swin [23], this windowed attention strategy is non-overlapping.\n\nMAE Pre-train We initialize the ViT with parameters pre-trained on ImageNet-1k [5] with Masked Auto Encoder (MAE) [14]. This self-supervised method can alleviate the over-fitting problem and helps the model generalize, supported by Table 9.\n\nMore specifically, we represent input images as $X \\in$ $\\mathbb{R}^{3 \\times h \\times w}$, and ground truth masks as $M \\in \\mathbb{R}^{1 \\times h \\times w}$, where $h$ and $w$ correspond to the height and width of the image, respectively. We then pad them to $X_{p} \\in \\mathbb{R}^{3 \\times H \\times W}$ and $M_{p} \\in \\mathbb{R}^{1 \\times H \\times W}$. Balance with computational cost and the resolution of datasets we employ in Table 2, we take $H=W=1024$ as constants in our implementation. Then $X_{p}$ is passed into the windowed ViT-Base encoder with 12 layers, with a complete global attention block retained every 3 layers. The above process can be formulated as follows:\n\n$$\nG_{e}=\\mathcal{V}\\left(X_{p}\\right) \\in \\mathbb{R}^{768 \\times \\frac{H}{16} \\times \\frac{W}{16}}\n$$\n\nwhere $\\mathcal{V}$ denotes the ViT, and $G_{e}$ stands for encoded feature map. The number of channels, 768, is to keep the information density the same as the RGB image at the input, as $768 \\times \\frac{H}{16} \\times \\frac{W}{16}=3 \\times H \\times W$.\n\n### 3.2 Simple Feature Pyramid Network\n\nTo introduce multi-scale supervision, we adopt the simple feature pyramid network (SFPN) after the ViT encoder, which was suggested in ViTDet [40]. This method takes the single output feature map $G_{e}$ from ViT, and then uses a series of convolutional and deconvolutional layers to perform up-sampling and down-sampling to obtain multi-scale feature maps:\n\n$$\nF_{i}=\\mathcal{C}_{i}\\left(G_{e}\\right) \\in \\mathbb{R}^{C_{S} \\times \\frac{H}{2^{i+2}} \\times \\frac{W}{2^{i+2}}}, i \\in\\{1,2,3,4\\}\n$$\n\nWhere $\\mathcal{C}_{i}$ denotes the convolution series, and $C_{S}$ is the output channel dimension for each layer in SFPN. This multi-scale method does not change the base structure of ViT, which allowed us to easily introduce recently advanced algorithms to the backbone.\n\n### 3.3 Light-weight Predict Head\n\nFor the final prediction, we aimed to design a model that is simple enough to reduce memory consumption while also demonstrating that the improvements come from the advanced design in the ViT Encoder and the multi-scale supervision. Based on these ideas, we adopted the decoder design from SegFormer [37], which outputs a smaller predicted mask $M_{e}$ with a resolution of $1 \\times \\frac{H}{4} \\times \\frac{W}{4}$. The lightweight all-MLP decoder first applies a linear layer to unify the channel dimension. It then up-samples all the features to the same resolution of $C_{D} \\times \\frac{H}{4} \\times \\frac{W}{4}$ with bilinear interpolation, and concatenates all the features together. Finally, a series of linear layers is applied to fuse all the layers and make the final prediction. We can formulate the prediction head as follows:\n\n$$\nP=M L P\\left\\{\\odot_{i}\\left(W_{i} F_{i}+b_{i}\\right)\\right\\} \\in \\mathbb{R}^{\\frac{H}{4} \\times \\frac{W}{4} \\times 1}\n$$",
      "has_image": false
    },
    {
      "page_number": 5,
      "markdown": "![img-2.jpeg](img-2.jpeg)\n\nFigure 3. Diagrams of the predict-head. The rectangles on the left represent the output of SFPN. There is a normalization layer before entering the MLP block, which is fully discussed below.\n\nHere, $P$ represents the predicted probability map for the manipulated area; $\\odot$ denotes concatenation operation, and $MLP$ refers to an MLP module. Detailed structure and analysis are illustrated in Figure 3.\n\n### 3.4. Edge Supervision Loss\n\nTo account for the fact that artifacts are typically more prevalent at the edges of tampered regions, where the differences between manipulated and authentic areas are most noticeable, we developed a strategy that places greater emphasis on the boundary region of the manipulated area. Specifically, we generate a binary edge mask $M^{\\star}$ from the original mask image $M$ using mathematical morphology operations including dilation ($\\odot$) and erosion ($\\ominus$) [32], followed by taking the absolute values of the result. The formula we use to generate the edge mask is:\n\n$$M^{\\star} = |(M \\ominus B(k)) - (M \\oplus B(k))| \\tag{4}$$\n\nwhere, $B(x)$ generates a $(2x + 1) \\times (2x + 1)$ cross matrix, where only the $x^{th}$ column and $x^{th}$ row have a value of 1, while the rest of the matrix contains 0s. The integer value $x$ is selected to be approximately equal to the width of the white area in the boundary mask. Examples of the edge mask generated using this approach are shown in Figure 4.\n\n**Combined Loss** To compute the loss function, we first pad the ground-truth mask $M$ and the edge mask $M^{\\star}$ to the size of $H \\times W$, and refer to them as $M_p$ and $M_p^\\star$, respectively. We then calculate the final loss using the following formula:\n\n$$\\mathcal{L} = \\mathcal{L} \\text{seg}(P, M_p) + \\lambda \\cdot \\mathcal{L} \\text{edge}(P \\ast M_p^\\star, M_p \\ast M_p^\\star) \\tag{5}$$\n\nwhere $\\ast$ denotes the point-wise product, which masks the original image. Both $\\mathcal{L} seg$ and $\\mathcal{L} edge$ are binary cross-entropy loss functions, and $\\lambda$ is a hyper-parameter that controls the balance between the segmentation and edge detection losses. By default, we searched the optimal $\\lambda = 20$ to guide the model to focus on the edge regions, which is supported by Figure 5. We choose a larger value for $\\lambda$ also for two reasons: (1) to emphasize the boundary region, and (2) to balance the significant number of zeros introduced by zero-padding.\n\nWhile the proposed edge loss strategy is straightforward, as we will discuss in the Experiments section (Figure 8), it\n\n![img-3.jpeg](img-3.jpeg)\n\nFigure 4. Examples of generating the edge mask $M^{\\star}$. White region represents for manipulated area, $k$ is set to 7 while the image size is 1024×682. The absolute value operation ensures that whether the tampered region dominates or the non-tampered region dominates, the mask only emphasizes the junction of the two.\n\n![img-4.jpeg](img-4.jpeg)\n\nFigure 5. Lambda selection, trained/test on CASIAv2/v1.\n\nTable 2. All datasets in our experiments.\n\n|  Dataset | Type |  | Manipulation type |  |  | Resolution |   |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n|   | Authentic | Manipulated | copynet | split | inpa | min | max  |\n|  CASIAv2 [7] | 7491 | 5123 | 3274 | 1849 | 0 | 240 | 800  |\n|  CASIAv1 [7] | 800 | 920 | 459 | 461 | 0 | 256 | 384  |\n|  NIST16 [10] | 0 | 564 | 68 | 288 | 208 | 480 | 5616  |\n|  COVERAGE [35] | 100 | 100 | 100 | 0 | 0 | 158 | 572  |\n|  Defacto-CIL [27] | 6000 | 6000 | 2000 | 2000 | 2000 | 120 | 640  |\n|  Columbia [35] | 183 | 180 | 0 | 180 | 0 | 568 | 1152  |\n|  IMD-20 [29] | 415 | 2010 | - | - | - | 176 | 4437  |\n|  tampCOCO [18] | 0 | 800000 | 600000 | 200000 | 0 | 51 | 640  |\n|  JPEG RAISE [18] | 24462 | 0 | - | - | - | 1515 | 6159  |\n\nremarkably accelerates model convergence, stabilizes the training process, and mitigates potential NaN issues. Therefore, we consider this strategy a powerful prior knowledge for IML problems, deserving attention in future research.\n\n## 4. Experiments\n\n### 4.1. Experimental Setup\n\n**Evaluation barrier for IML** While recent studies have introduced numerous SoTA models, comparing them on an equal footing remains challenging. This is due to the following reasons: 1) lack of publicly available code for the models and training processes [2, 15]; 2) utilization of massive synthesized datasets that are inaccessible to the wider research community [34, 36, 39]; 3) training and testing datasets often vary across different papers, also bringing difficulty for comparison.\n\n**Datasets and Evaluation Protocol** To facilitate reproducibility and overcome the existing evaluation barrier, we demarcate existing mainstream IML methods into three distinct protocols based on different partitions of datasets. Subsequently, we compare IML-ViT against SoTA methods with these three protocols, as shown in Table 2 and Table 3. We followed MVSS-Net [2] to create Defacto-12k dataset. More details will be discussed in Section 4.2.\n\n[Figure 3] Figure 3. Diagrams of the predict-head. The rectangles on the\nleft represent the output of SFPN. There is a normalization layer\nbefore entering the MLP block, which is fully discussed below.  This figure is a model architecture diagram, likely representing a neural network or deep learning model. It includes several key components: \n\n- **Nodes**: Represented by boxes labeled \"Linear,\" \"UpSample,\" and \"Concatenate,\" indicating different layers or operations within the model.\n- **Connections**: Arrows between nodes show the flow of data through the model.\n- **Labels**: Text such as \"1/32,\" \"1/16,\" \"1/8,\" and \"1/4\" likely denote the resolution or dimensionality of the data at different stages.\n- **Concatenate**: This operation combines the outputs from the \"Linear\" and \"UpSample\" layers.\n- **MLP**: Likely stands for Multi-Layer Perceptron, a type of neural network, at the end of the architecture.\n\nThe figure mainly demonstrates the structure of a model that processes data at different resolutions, up-samples it, and then combines the results. The most noticeable numbers are the resolutions (1/32, 1/16, 1/8, 1/4), which indicate the model's ability to handle data at multiple scales.\n\n\n[Figure 4] Figure 5. Lambda selection, trained/test on CASIAv2/v1. Table 2. All datasets in our experiments.  This figure is a line graph. It displays the relationship between the \"Value of λ\" on the x-axis and the \"Pixel-level F1\" on the y-axis. The graph uses a red line to connect data points, which are marked with red circles. The x-axis ranges from 0 to 40, and the y-axis ranges from 0.4 to 0.7. The graph shows an initial upward trend as λ increases, peaking around λ = 10, after which the F1 score plateaus and then slightly decreases. The most noticeable feature is the peak at λ = 10, where the F1 score is highest.\n\n\n[Figure 5] (a) more manipulated region\n(b) more authentic region Figure 4. Examples of generating the edge mask M ⋆. White\nregion represents for manipulated area, k is set to 7 while the\nimage size is 1024×682. The absolute value operation ensures that\nwhether the tampered region dominates or the non-tampered region\ndominates, the mask only emphasizes the junction of the two.  The figure is an output example, showing the results of a segmentation or contour detection process on a silhouette image of a baseball player holding a bat. The left side of the figure displays the original silhouette, while the right side shows the refined or enhanced contour of the same silhouette. The visual elements include a white silhouette on a black background, with a yellow arrow indicating the direction of the transformation. The figure demonstrates the improvement in the contour's definition and smoothness after the processing. No specific numbers, thresholds, or units are present in the figure. The most noticeable difference is the enhanced clarity and smoothness of the contour on the right compared to the original on the left.\n\n\n[Figure 6] (a) more manipulated region\n(b) more authentic region Figure 4. Examples of generating the edge mask M ⋆. White\nregion represents for manipulated area, k is set to 7 while the\nimage size is 1024×682. The absolute value operation ensures that\nwhether the tampered region dominates or the non-tampered region\ndominates, the mask only emphasizes the junction of the two.  This figure is an output example, showing the results of a silhouette extraction process. It consists of two panels: the left panel displays the original silhouette image of two people, and the right panel shows the extracted silhouette outlines. The figure demonstrates the effectiveness of the silhouette extraction algorithm, highlighting the clear separation of the two figures from the background. The color scheme is black and white, with the original image in grayscale and the extracted outlines in white on a black background. There are no axes, labels, or legends present, as this is a visual representation of the algorithm's output rather than a data visualization. The most noticeable feature is the improved clarity and separation of the two figures in the extracted silhouette on the right.\n",
      "has_image": false
    },
    {
      "page_number": 6,
      "markdown": "Table 3: The protocols we demarcate from the existing works.\n\n| Protocol | Details | Seminal paper |\n| --- | --- | --- |\n| No.1 | Train on CASIAv2. Test on other small datasets. | MVSS-Net [2] |\n| No.2 | Train on six large mixed datasets. Test on other small datasets. | CAT-Netv2 [18] |\n| No.3 | Random split train/test dataset on mixed small datasets. | ObjectFormer [34] |\n\nTable 4: Complexity of IML-ViT compared to open-sourced SoTA models. Inference time is measured on a per-image basis.\n\n| Method | Infer. Time(s) | Params.(M) | 512×512 FLOPs(G) | 1024×1024 FLOPs(G) |\n| --- | --- | --- | --- | --- |\n| MVSS-Net*[2, 6]* | 2.929 | 147 | 167 | 683 |\n| PSCC-Net [22] | 0.072 | 3 | 120 | 416 |\n| HiFi-Net [12] | 1.512 | 7 | 404 | 3470 |\n| TruFor [11] | 1.231 | 68 | 231 | 1016 |\n| IML-ViT | 0.094 | 91 | 136 | 445 |\n\nEvaluation Criteria We evaluate our model using pixel-level $F_{1}$ score with a fixed threshold $0.5$ and Area Under the Curve (AUC), which are commonly used evaluation metrics in previous works. Both of them are metrics where higher values indicate better performance. However, it’s worth noting that AUC can be influenced by excessive truenegative pixels in IML datasets, leading to an overestimation of model performance. Nevertheless, our model achieves SoTA performance in both $F_{1}$ score and AUC.\n\nImplementation We pad all images to a resolution of 1024x1024, except for those that exceed this limit. For the larger images, we resized them to the longer side to 1024 and maintained their aspect ratio. During training, following MVSS-Net [2], common data augmentation techniques were applied, including re-scaling, flipping, blurring, rotation, and various naive manipulations (e.g., randomly copy-moving or inpainting rectangular areas within a single image). We used the AdamW optimizer [25] with a base learning rate of 1e-4, scheduled with a cosine decay strategy [24]. The early stop technique was employed during training.\n\nComplexity Training IML-ViT with a batch size of 2 per GPU consumed 22GB of GPU memory per card. Using four NVIDIA 3090 GPUs, the model was trained on a dataset of 12,000 images over 200 epochs, taking approximately 12 hours. For inference, a batch size of 6 per GPU required 20GB of GPU memory, with an average prediction time of 0.094 seconds per image. Reducing the batch size to 1 decreased the GPU memory requirement to 5.4GB. We also compare the number of parameters and FLOPs with SoTA models in Table 4 and achieve highly competitive results.\n\n### 4.2 Compare with SoTA (See Table 3 for protocols)\n\nProtocol No.1 Since MVSS-Net [2] has already conducted a detailed evaluation on a fair cross-dataset protocol and later works [41] followed their setting, we directly quote their results here and train our models with the same protocol. The results measured by $F_{1}$ score are listed respectively in Table 5. We also compare this with some closed-source methods that only report their AUC tested on CASIAv1 in\n\n![img-5.jpeg](img-5.jpeg)\n\nFigure 6: Qualitative results on Protocol No.1 of IML-ViT compared to ManTra-Net and MVSS-Net. For More results, see Appendix.\n\n![img-6.jpeg](img-6.jpeg)\n\nFigure 7: Qualitative results of IML-ViT for ablation Study. We remove each component to test their contribution.\n\n![img-7.jpeg](img-7.jpeg)\n\nFigure 8: Training stability influenced by proposed edge loss.\n\nTable 7: Overall, our model achieves SoTA performance on this cross-dataset evaluation protocol. Figure 6 illustrates that our model portrays high-quality and clear edges under different preferences of manipulation types.\n\nProtocol No.2 TruFor [11] is a recent strong method with extensive experimental results, training on six relatively large IML datasets proposed by CAT-Netv2 [18]. In our aim to establish IML-ViT as the benchmark model, we adopt their protocol to compare our model. We outperform them on four benchmark datasets. Details are shown in Table 8.\n\nProtocol No.3 TransForensic [13], ObjectFormer [34], HiFi-Net [12] and CFL-Net [28] reported their performance based on mixed datasets. They randomly split these datasets\n\n[Figure 7] Figure 6. Qualitative results on Protocol No.1 of IML-ViT com-\npared to ManTra-Net and MVSS-Net.\nFor More results, see\nAppendix.  This figure is an output example of a model's performance on various images. It shows the original images on the left, followed by the model's predictions (white silhouettes) and the ground truth (black silhouettes) in the middle and right columns, respectively. The images depict diverse scenes, including people, animals, and objects. The figure demonstrates the model's ability to segment and identify objects in different contexts. No specific numbers, thresholds, or units are visible, but the contrast between the model's predictions and the ground truth highlights the model's accuracy and areas for improvement.\n\n\n[Figure 8] Figure 7. Qualitative results of IML-ViT for ablation Study. We\nremove each component to test their contribution.  This figure is an output example, showcasing the results of a model under different configurations. It includes multiple columns labeled \"w/o multi-scale,\" \"w/o MAE,\" \"w/o high-reso,\" \"w/o edge loss,\" \"Full setup,\" \"Groundtruth,\" and \"Image.\" Each column contains grayscale images, with the \"Groundtruth\" and \"Image\" columns showing the original and actual images, respectively. The \"Full setup\" column displays the best results, while the \"w/o multi-scale\" column shows the least accurate results. The figure demonstrates the impact of different configurations on the model's performance, with the \"Full setup\" achieving the most accurate results.\n\n\n[Figure 9] Figure 8. Training stability influenced by proposed edge loss.  This figure is a line graph. It displays the F1 score on CASIAv1 as a function of training epochs. The x-axis represents the number of training epochs, ranging from 0 to 100, while the y-axis shows the F1 score, which ranges from 0.0 to 0.6. The graph includes three distinct lines, each represented by a different marker and color: blue squares for \"w/ edge loss,\" orange circles for \"w/o edge loss (1),\" and green triangles for \"w/o edge loss (2).\" The legend clarifies the meaning of each marker. The figure demonstrates the performance of different models with and without edge loss during training. The most noticeable difference is the steady increase in the F1 score for the \"w/ edge loss\" model, which outperforms the other two models throughout the training epochs.\n",
      "has_image": false
    },
    {
      "page_number": 7,
      "markdown": "| Method | Pixel-level $F_{1}$ score | | | | | | |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| | CASIAv1 | Columbia | NIST16 | Coverage | Defacto-12k | MEAN | |\n| HP-FCN*, ICCV19 [19] | 0.154 | 0.067 | 0.121 | 0.003 | 0.055 | 0.080 | |\n| ManTra-Net*, CVPR19 [36] | 0.155 | 0.364 | 0.000 | 0.286 | 0.155 | 0.192 | |\n| CR-CNN*, ICME20 [38] | 0.405 | 0.436 | 0.238 | 0.291 | 0.132 | 0.300 | |\n| GSR-Net*, AAAI20 [44] | 0.387 | 0.613 | 0.283 | 0.285 | 0.051 | 0.324 | |\n| MVSS-Net*, ICCV21 [2] | 0.452 | 0.638 | 0.292 | 0.453 | 0.137 | 0.394 | |\n| MVSS-Net (re-trained) | 0.435 | 0.303 | 0.203 | 0.329 | 0.097 | 0.270 | |\n| MVSS-Net++*, PAMI22 [6] | 0.513 | 0.660 | 0.304 | $\\mathbf{0 . 4 8 2}$ | 0.095 | 0.411 | |\n| NCL-IML, ICCV23 [41] | 0.598 | 0.704 | 0.231 | 0.383 | 0.066 | 0.396 | |\n| IML-ViT (ours) | $\\mathbf{0 . 7 2 1}$ | $\\mathbf{0 . 7 8 0}$ | $\\mathbf{0 . 3 3 1}$ | 0.410 | $\\mathbf{0 . 1 5 6}$ | $\\mathbf{0 . 4 8 0}$ | |\n\nTable 5. Evaluation results of Protocol No.1. Except for ManTra-Net and HP-FCN, which were trained on a privately synthesized dataset, all the methods were trained on CASIAv2 datasets. The best scores are highlighted in bold. Symbol \"*\" marks the results are quoted from MVSS-Net paper [2].\n\nTable 6. Evaluation results of Protocol No.3. $*$ marks cross-dataset results. Metrics are quoted.\n\n| Method | Datasets (Train/validate/test split) | Pixel-level AUC | | | | | Pixel-level $F 1$ | | | | | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | | | | COVER | NIST16 | CASIA | IMD-20 | CASIA | COVER | | TransForesinc, ICCV21 | COVER + CASIA + IMD20 (8:1:1) | 0.884 | - | 0.850 | 0.848 | 0.627 | 0.674 | | IML-ViT(Ours) | COVER + CASIA + IMD20 (8:1:1) | 0.912 | $0.821^{*}$ | $\\mathbf{0 . 9 6 1}$ | $\\mathbf{0 . 9 4 3}$ | $\\mathbf{0 . 8 2 5}$ | $\\mathbf{0 . 8 1 5}$ | | ObjectFormer, CVPR22 | COVER(4:1); NIST(4:1); CASIA(v2:v1) | 0.957 | 0.996 | 0.882 | - | 0.579 | 0.758 | | Hifi-Net, CVPR23 | COVER(4:1); NIST(4:1); CASIA(v2:v1) | $\\mathbf{0 . 9 6 1}$ | 0.996 | 0.885 | - | 0.616 | 0.801 | | CFL-Net , WACV23 | NIST16 + CASIA + IMD20 (8:1:1) | - | $\\mathbf{0 . 9 9 7}$ | 0.863 | 0.899 | - | - | | IML-ViT(Ours) | NIST16 + CASIA + IMD20 (8:1:1) | $0.801^{*}$ | $\\mathbf{0 . 9 9 7}$ | $\\mathbf{0 . 9 5 9}$ | $\\mathbf{0 . 9 4 1}$ | $\\mathbf{0 . 8 2 0}$ | $0.505^{*}$ | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |",
      "has_image": false
    },
    {
      "page_number": 8,
      "markdown": "Table 9: Ablation study of IML-ViT. Each model is trained for 200 epochs on the CASIAv2 dataset. Best scores are marked in bold. H-Reso refers to high resolution; SFPN refers to simple feature pyramid network; and Edge refers to proposed edge supervision.\n\n| Test Goal | Init Method | Components | | | CASIAv1 | | Coverage | | Columbia | | NIST16 | | MAEN | |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| | | H-Reso | SFPN | Edge | F1 | AUC | F1 | AUC | F1 | AUC | F1 | AUC | F1 | AUC |\n| w/o MAE | Xavier | $+$ | $+$ | $+$ | 0.1035 | - | 0.0439 | - | 0.0744 | - | 0.0632 | - | 0.0713 | - |\n| | ViT-B ImNet-21k | $+$ | $+$ | $+$ | 0.5820 | 0.9037 | 0.2123 | 0.7898 | 0.5040 | 0.8335 | 0.2453 | 0.7939 | 0.3859 | 0.8302 |\n| w/o high resolution | MAE ImNet-1k | - | $+$ | $+$ | 0.5747 | 0.9121 | 0.2622 | 0.7889 | 0.5150 | 0.8028 | 0.3292 | 0.7950 | 0.4153 | 0.8247 |\n| w/o multi-scale | MAE ImNet-1k | $+$ | - | $+$ | 0.6504 | 0.9306 | 0.3877 | 0.8829 | 0.7096 | 0.8816 | 0.2847 | 0.7771 | 0.5081 | 0.8681 |\n| w/o edge-supervision | MAE ImNet-1k | $+$ | $+$ | - | 0.6177 | 0.9240 | 0.3176 | 0.8789 | 0.6843 | 0.9161 | 0.2648 | 0.8045 | 0.4711 | 0.8809 |\n| Full setup | MAE ImNet-1k | $+$ | $+$ | $+$ | 0.7206 | 0.9420 | 0.4099 | 0.9137 | 0.7798 | 0.9337 | 0.3317 | 0.8064 | 0.5605 | 0.8990 |\n\n![img-8.jpeg](img-8.jpeg)\n\nFigure 9. Robustness evaluation by three kinds of attacking across various SoTA methods under Protocol No.1.\n\nThe initialization with Imagenet-21k achieves acceptable performance on CASIAv1, which is homologous to CASIAv2, it exhibits poor generalization ability on other non-homology datasets. This indicates that MAE greatly alleviates the problem of non-convergence and over-fitting of ViT on limited IML datasets.\n\nEdge supervision is crucial. The performance of IML-ViT without edge loss shows significant variability with different random seeds, all leading to gradient collapse eventually, where the F1 score reaches 0, and the loss becomes *NaN*, as shown in Figure 8. In contrast, when employing edge loss, all performance plots exhibit consistent behavior similar to the blue line in Figure 8, enabling fast convergence and smooth training up to 200 epochs. Furthermore, Table 9 confirms the effectiveness of edge loss in contributing to the final performance. In summary, these results demonstrate that edge supervision effectively stabilizes IML-ViT convergence and can serve as highly efficient prior knowledge for IML problems.\n\nHigh resolution is effective for artifacts. The improved performance shown in Table 9 for the *full setup* model across four datasets validates the effectiveness of the high-resolution strategy. However, it is essential to note that the NIST16 dataset shows limited improvement when using higher resolutions. This observation can be attributed to the fact that the NIST16 dataset contains numerous images with resolutions exceeding 2000, and down-sampling these images to 1024 for testing may lead to considerable distortion of the original artifacts, consequently reducing the effectiveness of learned features. Nevertheless, when considering the SoTA score achieved, it becomes evident that IML-ViT can flexibly infer the manipulated area based on the richness of different information types.\n\nMulti-scale supervision helps generalize. All these datasets exhibit significant variations in the proportion of manipulated area, particularly where CASIAv2 has 8.96% of the pixels manipulated, COVERAGE dataset has 11.26%, Columbia dataset has 26.32%, and NIST16 has 7.54%. Nevertheless, the comprehensive improvements in Table 9 with the aid of multi-scale supervision indicate that this technique can effectively bridge the gap in dataset distribution, enhancing generalization performance.\n\n### 4.4 Robustness Evaluation\n\nWe conducted a robustness evaluation on our IML-ViT model following MVSS-Net [2]. We utilized their protocol with three common types of attacks, including JPEG compression, Gaussian Noise, and Gaussian Blur. As shown in Figure 9, IML-ViT achieved very competitive results among SoTA models, which proved to possess excellent robustness.\n\n## 5 Conclusions\n\nThis paper introduces IML-ViT, the first image manipulation localization model based on ViT. Extensive experiments on three mainstream protocols demonstrate that IML-ViT achieves SoTA performance and generalization ability, validating the reliability of the three core elements of the IML task proposed in this study: high resolution, multi-scale, and edge supervision. Further, IML-ViT proves the effectiveness of self-attention in capturing non-semantic artifacts. Its simple structure makes it a promising benchmark for IML.",
      "has_image": false
    },
    {
      "page_number": 9,
      "markdown": "## References\n\n[1] Belhassen Bayar and Matthew C. Stamm. Constrained convolutional neural networks: A new approach towards general purpose image manipulation detection. IEEE Transactions on Information Forensics and Security, 13(11):2691-2706, 2018. 3\n[2] Xinru Chen, Chengbo Dong, Jiaqi Ji, Juan Cao, and Xirong Li. Image manipulation detection by multi-view multi-scale supervision. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), page 14165-14173, Montreal, QC, Canada, 2021. IEEE. 2, 3, 5, 6, 7, 8, 11, 12\n[3] Davide Cozzolino, Giovanni Poggi, and Luisa Verdoliva. Splicebuster: A new blind image splicing detector. In 2015 IEEE International Workshop on Information Forensics and Security (WIFS), page 1-6, Roma, Italy, 2015. IEEE. 3\n[4] Davide Cozzolino, Giovanni Poggi, and Luisa Verdoliva. Efficient dense-field copy-move forgery detection. IEEE Transactions on Information Forensics and Security, 10(11): 2284-2297, 2015. 3\n[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, page 248-255, Miami, FL, 2009. IEEE. 2, 4\n[6] Chengbo Dong, Xinru Chen, Ruohan Hu, Juan Cao, and Xirong Li. Mvss-net: Multi-view multi-scale supervised networks for image manipulation detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, page 1-14, 2022. 6, 7\n[7] Jing Dong, Wei Wang, and Tieniu Tan. Casia image tampering detection evaluation database. In 2013 IEEE China Summit and International Conference on Signal and Information Processing, page 422-426, Beijing, China, 2013. IEEE. 2,5\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. (arXiv:2010.11929), 2021. arXiv:2010.11929 [cs]. 2\n[9] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256. JMLR Workshop and Conference Proceedings, 2010. 7\n[10] Haiying Guan, Mark Kozak, Eric Robertson, Yooyoung Lee, Amy N. Yates, Andrew Delgado, Daniel Zhou, Timothee Kheyrkhah, Jeff Smith, and Jonathan Fiscus. Mfc datasets: Large-scale benchmark datasets for media forensic challenge evaluation. In 2019 IEEE Winter Applications of Computer Vision Workshops (WACVW), page 63-72, Waikoloa Village, HI, USA, 2019. IEEE. 5\n[11] Fabrizio Guillaro, Davide Cozzolino, Avneesh Sud, Nicholas Dufour, and Luisa Verdoliva. Trufor: Leveraging all-round clues for trustworthy image forgery detection and localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20606-20615, 2023. $2,3,6,7,11$\n[12] Xiao Guo, Xiaohong Liu, Zhiyuan Ren, Steven Grosz, Iacopo Masi, and Xiaoming Liu. Hierarchical fine-grained image forgery detection and localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3155-3165, 2023. 2, 6\n[13] Jing Hao, Zhixin Zhang, Shicai Yang, Di Xie, and Shiliang Pu. Transforensics: Image forgery localization with dense self-attention. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), page 15035-15044, Montreal, QC, Canada, 2021. IEEE. 2, 3, 6\n[14] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 15979-15988, New Orleans, LA, USA, 2022. IEEE. 2, 4\n[15] Xuefeng Hu, Zhihan Zhang, Zhenye Jiang, Syomantak Chaudhuri, Zhenheng Yang, and Ram Nevatia. Span: Spatial pyramid attention network for image manipulation localization. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXI 16, pages 312-328. Springer, 2020. 2, 3, 5, 7\n[16] Minyoung Huh, Andrew Liu, Andrew Owens, and Alexei A. Efros. Fighting Fake News: Image Splice Detection via Learned Self-Consistency, page 106-124. Springer International Publishing, Cham, 2018. 3\n[17] Vladimir V Kniaz, Vladimir Knyaz, and Fabio Remondino. The point where reality meets fantasy: Mixed adversarial generators for image splice detection. 3\n[18] Myung-Joon Kwon, Seung-Hun Nam, In-Jae Yu, Heung-Kyu Lee, and Changick Kim. Learning jpeg compression artifacts for image manipulation detection and localization. International Journal of Computer Vision, 130(8):1875-1895, 2022. $3,6,7$\n[19] Haodong Li and Jiwu Huang. Localization of deep inpainting using high-pass fully convolutional network. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), page 8300-8309, Seoul, Korea (South), 2019. IEEE. 7\n[20] Yanghao Li, Saining Xie, Xinlei Chen, Piotr Dollar, Kaiming He, and Ross Girshick. Benchmarking detection transfer learning with vision transformers. (arXiv:2111.11429), 2021. arXiv:2111.11429 [cs]. 2, 4\n[21] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Mvitv2: Improved multiscale vision transformers for classification and detection. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 4794-4804, New Orleans, LA, USA, 2022. IEEE. 4\n[22] Xiaohong Liu, Yaojie Liu, Jun Chen, and Xiaoming Liu. Psccnet: Progressive spatio-channel correlation network for image manipulation detection and localization. IEEE Transactions on Circuits and Systems for Video Technology, 2022. 6\n[23] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), page 9992-10002, Montreal, QC, Canada, 2021. IEEE. 4",
      "has_image": false
    },
    {
      "page_number": 10,
      "markdown": "[24] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. (arXiv:1608.03983), 2017. arXiv:1608.03983 [cs, math]. 6\n[25] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. (arXiv:1711.05101), 2019. arXiv:1711.05101 [cs, math]. 6\n[26] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive field in deep convolutional neural networks. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2016. 2\n[27] Gael Mahfoudi, Badr Tajini, Florent Retraint, Frederic Morain-Nicolier, Jean Luc Dugelay, and Marc Pic. Defacto: Image and face manipulation dataset. In 2019 27th European Signal Processing Conference (EUSIPCO), page 1-5, A Coruna, Spain, 2019. IEEE. 2, 5\n[28] Fahim Faisal Niloy, Kishor Kumar Bhaumik, and Simon S. Woo. Cfl-net: Image forgery localization using contrastive learning. In 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), page 4631-4640, Waikoloa, HI, USA, 2023. IEEE. 6, 12, 13\n[29] Adam Novozamsky, Babak Mahdian, and Stanislav Saic. Imd2020: A large-scale annotated dataset tailored for detecting manipulated images. In 2020 IEEE Winter Applications of Computer Vision Workshops (WACVW), page 71-80, Snowmass Village, CO, USA, 2020. IEEE. 5\n[30] Yuan Rao and Jiangqun Ni. A deep learning approach to detection of splicing and copy-move forgeries in images. In 2016 IEEE International Workshop on Information Forensics and Security (WIFS), page 1-6, Abu Dhabi, United Arab Emirates, 2016. IEEE. 3\n[31] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Gradcam: Visual explanations from deep networks via gradientbased localization. In Proceedings of the IEEE international conference on computer vision, pages 618-626, 2017. 12\n[32] Jean Serra. Image analysis and mathematical morphology, 1983. 5\n[33] Luisa Verdoliva. Media forensics and deepfakes: An overview. IEEE Journal of Selected Topics in Signal Processing, 14(5): 910-932, 2020. 1\n[34] Junke Wang, Zuxuan Wu, Jingjing Chen, Xintong Han, Abhinav Shrivastava, Ser-Nam Lim, and Yu-Gang Jiang. Objectformer for image manipulation detection and localization. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 2354-2363, New Orleans, LA, USA, 2022. IEEE. 2, 3, 5, 6, 7, 12, 13\n[35] Bihan Wen, Ye Zhu, Ramanathan Subramanian, Tian-Tsong Ng, Xuanjing Shen, and Stefan Winkler. Coverage - a novel database for copy-move forgery detection. In 2016 IEEE International Conference on Image Processing (ICIP), page 161-165, Phoenix, AZ, USA, 2016. IEEE. 5\n[36] Yue Wu et al. Mantra-net: Manipulation tracing network for detection and localization of image forgeries with anomalous features. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 9535-9544, Long Beach, CA, USA, 2019. IEEE. 1, 2, 3, 5, 7\n[37] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in neural information processing systems, 34:1207712090, 2021. 3, 4\n[38] Chao Yang, Huizhou Li, Fangting Lin, Bin Jiang, and Hao Zhao. Constrained r-cnn: A general image manipulation detection model. In 2020 IEEE International conference on multimedia and expo (ICME), page 1-6. IEEE, 2020. 2, 3, 7\n[39] Chao Yang, Zhiyu Wang, Huawei Shen, Huizhou Li, and Bin Jiang. Multi-modality image manipulation detection. In 2021 IEEE International Conference on Multimedia and Expo (ICME), pages 1-6, Shenzhen, China, 2021. IEEE. 2, 5\n[40] Li Yanghao, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. (arXiv:2203.16527), 2022. arXiv:2203.16527 [cs]. 4\n[41] Jizhe Zhou, Xiaochen Ma, Xia Du, Ahmed Y Alhammadi, and Wentao Feng. Pre-training-free image manipulation localization through non-mutually exclusive contrastive learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22346-22356, 2023. 3, 6, 7\n[42] Peng Zhou, Xintong Han, Vlad I. Morariu, and Larry S. Davis. Learning rich features for image manipulation detection. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, page 1053-1061, Salt Lake City, UT, USA, 2018. IEEE. 7\n[43] Peng Zhou, Xintong Han, Vlad I. Morariu, and Larry S. Davis. Learning rich features for image manipulation detection. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, page 1053-1061, Salt Lake City, UT, USA, 2018. IEEE. 3\n[44] Peng Zhou, Bor-Chun Chen, Xintong Han, Mahyar Najibi, Abhinav Shrivastava, Ser-Nam Lim, and Larry Davis. Generate, segment, and refine: Towards generic manipulation segmentation. Proceedings of the AAAI Conference on Artificial Intelligence, 34(07):13058-13065, 2020. 2, 7\n[45] Xinshan Zhu, Yongjun Qian, Xianfeng Zhao, Biao Sun, and Ya Sun. A deep learning approach to patch-based image inpainting forensics. Signal Processing: Image Communication, 67:90-99, 2018. 3",
      "has_image": false
    },
    {
      "page_number": 11,
      "markdown": "## Appendix A Futher Robustness Evaluation\n\nJPEG compression, Gaussian Noise, and Gaussian Blur are the common attack methods for Image manipulation localization. Following the convention from TruFor [11] and MVSS-Net [2], we further carried out experiments on the resistance of these operations on Protocol No.1 and No.2. The evaluation results are shown in Figure 10 and Figure 11. The IML-ViT exhibited excellent resistance to these attack methods and consistently maintained the best performance of the models.\n\n![img-9.jpeg](img-9.jpeg)\n\nFigure 10. Robustness evaluation against common attack on Protocol No.2. Results are quoted from TruFor paper. They searched for the optimal F1 score to report the results while we selected 0.5 as the threshold for the F1 score here, proving IML-ViT more suitable for real-world scenarios.\n\n![img-10.jpeg](img-10.jpeg)\n\nFigure 11. Robustness Evaluation against JPEG compression and Gaussian blur on Protocol No.1. The red dashed line represents the F1 score when all predictions are classified as positive. When the result is lower than this line, we consider the model to be less effective than random guessing and losing its localization ability. Performance against JPEG compression is quoted from MVSS-Net, while performance against Gaussian Blur is retested by us using three publicly available models. Our model has a later entry of the F1 score into the red-line value compared to other models, and it consistently maintains a relatively high position, proving its better resistance.\n\n## Appendix B More Implementation Details\n\n### B.1 High-resolution ViT-Base\n\nMostly following the original Vision Transformer, we implemented our model with a stack of Transformer blocks stacking together. LN are employed in the self-attention head and MLP blocks. Every two windowed attention blocks are followed by a global attention block. The windowed attention block only computes self-attention in a small, non-overlapped window, while the global attention block ensures global information propagation. Although we introduce the windowed attention, it only affects the self-attention manner but doesn't change the linear projection for Q, K, and V. Therefore, we can directly apply the MAE pre-trained parameters from a vanilla ViT-B with all global attention to this windowed ViT-B without any extra process. Detailed configuration are shown in Table 10.\n\n|  Configs | Value  |\n| --- | --- |\n|  patch size | 16  |\n|  embedding dim | 768  |\n|  depth | 12  |\n|  number of heads | 12  |\n|  input size | 3×1024×1024  |\n|  window size | 14  |\n|  Norm layer | LN  |\n|  Global block indexes | 2,5,8,11  |\n|  Output shape | 768×64×64  |\n\nTable 10. Detailed structure of windowed ViT-Base\n\n### B.2 Simple Feature Pyramid\n\nAfter obtaining the output from ViT-B, SFPN utilizes a sequence of convolutional, pooling, or deconvolutional (ConvTranspose2D) layers to downsample it into feature maps with 256 channels, scaling them to resolutions of {4.0, 2.0, 1.0, 0.5, 0.25} relative to the resolution of the input feature maps (768×64×64). For example, the largest output feature map with a scale of 4.0 is shaped like 256×256×256, while the smallest one with a scale of 0.25 is shaped like 256×8×8. Each layer is followed by LayerNorm. Detailed structures of each scale can be seen in Table 11.\n\n|  Scales | Layers & channels of feature maps  |\n| --- | --- |\n|  4.0 | 768 ConvT 384 ConvT 192 Conv(1,1) 256 Conv(3,3) 256  |\n|  2.0 | 768 ConvT 384 Conv(1,1) 256 Conv(3,3) 256  |\n|  1.0 | 768 Conv(1,1) 256 Conv(3,3) 256  |\n|  0.5 | 768 maxpool2D 384 Conv(1,1) 256 Conv(3,3) 256  |\n|  0.25 | 768 maxpool2D 384 Conv(1,1) 256 Conv(3,3) 256 maxpool2D 256  |\n\nTable 11. Detailed structure of Simple feature pyramid ConvT denotes for ConvTranspose2D with kernel size of 2 and stride of 2; Conv(x, x) indicates that a Conv2D layer with kernel size of x; and maxpool2D has also a kernel size of 2. The number shown between layers indicates the number of channels for its respective feature map between layers.\n\n### B.3 Predict-head’s norm & training including authentic images\n\nThe exact structure we applied in the predict-head is shown in Figure 3. There is a norm layer before the last 1 × 1\n\n[Figure 10] Figure 10. Robustness evaluation against common attack on\nProtocol No.2. Results are quoted from TruFor paper. They\nsearched for the optimal F1 score to report the results while we\nselected 0.5 as the threshold for the F1 score here, proving IML-ViT\nmore suitable for real-world scenarios.  This figure is a set of line graphs comparing the F1 scores of different models under varying levels of Gaussian noise and JPEG compression. \n\nKey visual elements include:\n- Three line graphs side by side.\n- Each graph has an x-axis labeled \"Gaussian Noise\" and \"JPEG compression,\" and a y-axis labeled \"F1 Score.\"\n- Different models are represented by distinct markers and colors: CAT-Netv2 (blue circles), MVSS-Net (orange squares), TruFor (green triangles), and IML-ViT (red stars).\n- The legend identifies the models and their corresponding markers.\n- The x-axis values range from \"orig\" to 23 for Gaussian noise and JPEG compression.\n\nThe figure mainly demonstrates how the performance of these models degrades as the level of Gaussian noise and JPEG compression increases. The IML-ViT model (red stars) consistently outperforms the others, maintaining a higher F1 score across all noise and compression levels. The most noticeable difference is the steady decline in performance for all models as the noise and compression levels increase, with IML-ViT showing the least decline.\n\n\n[Figure 11] Figure 11. Robustness Evaluation against JPEG compression\nand Gaussian blur on Protocol No.1. The red dashed line repre-\nsents the F1 score when all predictions are classified as positive.\nWhen the result is lower than this line, we consider the model to\nbe less effective than random guessing and losing its localization\nability. Performance against JPEG compression is quoted from\nMVSS-Net, while performance against Gaussian Blur is retested\nby us using three publicly available models. Our model has a later\nentry of the F1 score into the red-line value compared to other\nmodels, and it consistently maintains a relatively high position,\nproving its better resistance.  This figure is a line graph. It displays the pixel-level F1 scores of three different models (ManTra-Net, CR-CNN, and GSR-Net) as the JPEG compression quality decreases from 100 to 50. The x-axis represents the JPEG compression quality, while the y-axis shows the pixel-level F1 scores. The graph includes a legend with symbols and colors for each model: ManTra-Net is represented by purple diamonds, CR-CNN by orange circles, and GSR-Net by green triangles. The figure demonstrates that as the compression quality decreases, the F1 scores for all models decline, with ManTra-Net maintaining the highest scores throughout. The red dashed line at approximately 0.15 represents a threshold for acceptable performance. The most prominent difference is the relative stability of ManTra-Net's performance compared to the other models.\n\n\n[Figure 12] Figure 11. Robustness Evaluation against JPEG compression\nand Gaussian blur on Protocol No.1. The red dashed line repre-\nsents the F1 score when all predictions are classified as positive.\nWhen the result is lower than this line, we consider the model to\nbe less effective than random guessing and losing its localization\nability. Performance against JPEG compression is quoted from\nMVSS-Net, while performance against Gaussian Blur is retested\nby us using three publicly available models. Our model has a later\nentry of the F1 score into the red-line value compared to other\nmodels, and it consistently maintains a relatively high position,\nproving its better resistance.  This figure is a line graph. It displays the pixel-level F1 scores of three different models: ManTra-Net, MVSS-Net, and IML-ViT, as a function of Gaussian blur kernel size. The x-axis represents the Gaussian blur kernel size, ranging from 0 to 29, while the y-axis represents the pixel-level F1 score, ranging from 0.0 to 0.7. The graph includes a legend with blue squares for ManTra-Net, red circles for MVSS-Net, and purple triangles for IML-ViT. A red dashed line at approximately 0.15 represents a threshold. The figure demonstrates how the performance of the models decreases as the Gaussian blur kernel size increases. MVSS-Net shows the highest initial performance but declines sharply, while ManTra-Net and IML-ViT maintain relatively stable performance until the kernel size reaches 17, after which their performance drops significantly.\n",
      "has_image": false
    },
    {
      "page_number": 12,
      "markdown": "convolution layer in the predict-head. We observed that when changing this layer may influence the following aspects: 1) convergence speed, 2) performance, and 3) generalizability.\n\nIn particular, Layer Norm can converge rapidly but is less efficient at generalization. Meanwhile, the Batch Norm can be generalized better on other datasets. However, when including authentic images during training, the Batch Norm may sometimes fail to converge. At present, a straightforward solution is to use Instance Normalization instead, which ensures certain convergence. Our experimental results are shown in Table 12.\n\nDelving into the reasons, MVSS-Net [2] is the pioneering paper proposing the incorporation of authentic images with fully black masks during training to reduce false positives. We highly endorse this conclusion as it aligns more closely with the practical scenario of filtering manipulated images from a massive dataset of real-world images. However, in terms of metrics in convention, because the F1 score is meaningful only for manipulated images (as there are no positive pixels for fully black authentic images, $F 1=\\frac{1.7 \\mathrm{TP}}{2 \\cdot 7 \\mathrm{TP}+1 \\mathrm{FP}+1 \\mathrm{FN}}$ yielding $F 1=0$ ), we only computed data for manipulated images. This approach may result in an \"unwarranted\" metric decrease when real images are included.\n\n|  Norm | Dataset | CASIAv1 |  | Coverage |  | Columbia |  | NIST16 |  | MEAN  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|   |  | F1 | Epoch | F1 | Epoch | F1 | Epoch | F1 | Epoch |   |\n|  Layer | CASIAv2-5k | 0.686 | 168 | 0.347 | 168 | 0.760 | 128 | 0.231 | 104 | 0.596  |\n|  Batch | CASIAv2-5k | 0.702 | 184 | 0.421 | 184 | 0.730 | 184 | 0.317 | 184 | 0.541  |\n|  Instance | CASIAv2-5k | 0.719 | 184 | 0.419 | 176 | 0.792 | 140 | 0.263 | 150 | 0.547  |\n|  Batch | CASIAv2-12k | 0.715 | 176 | 0.352 | 128 | 0.767 | 150 | 0.263 | 124 | 0.524  |\n|  Instance | CASIAv2-12k | 0.721 | 140 | 0.362 | 100 | 0.784 | 136 | 0.258 | 68 | 0.531  |\n\nTable 12. Testing for norm layer in predict-head Implementation is followed to ablation study in the main paper. CASIAv2-5k refers to manipulated images only, while 12 k includes authentic images as well.\n\nTraining settings Since our model could only train with small batch size, we applied the gradient accumulate method during training, i.e. updating the parameters every 8 images during training. We select this parameter by experiments, details see Table 13.\n\n|  Batchsize | GPUs | accum iter | CASIAv1 |  | Coverage |  | Columbia |  | NIST16 |  | MEAN  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|   |  |  | F1 | Epoch | F1 | Epoch | F1 | Epoch | F1 | Epoch |   |\n|  2 | 4 | 2 | 0.686 | 184 | 0.302 | 144 | 0.685 | 92 | 0.304 | 144 | 0.494  |\n|  2 | 4 | 4 | 0.704 | 192 | 0.386 | 140 | 0.772 | 60 | 0.331 | 140 | 0.548  |\n|  2 | 4 | 8 | 0.722 | 152 | 0.410 | 140 | 0.780 | 84 | 0.332 | 140 | 0.561  |\n|  2 | 4 | 16 | 0.706 | 184 | 0.419 | 184 | 0.782 | 92 | 0.314 | 184 | 0.555  |\n|  2 | 4 | 32 | 0.602 | 184 | 0.249 | 184 | 0.740 | 104 | 0.254 | 184 | 0.461  |\n\nTable 13. Test for best accumulate gradient parameter for IMLViT. Train/tested on CASIAv2/v1 with four NVIDIA 3090 GPUs.\n\nBesides, we adopt the early stop method during training. Evaluate the performance on the F1 score for CASIAv1, and stop training when there is no improvement for 15 epochs. Other configs are described in Table 14.\n\nTable 14. Training settings for IML-ViT\n\n|  Configs | Value  |\n| --- | --- |\n|  batch size | 2 (RTX 3090) or 4 (A40)  |\n|  GPU numbers | 4 (RTX 3090) or 2 (A40)  |\n|  accumulate gradient batch size | 8  |\n|  epochs | 200  |\n|  warm up epochs | 4  |\n|  optimizer | AdamW  |\n|  optimizer momentum | $\\beta_{1}, \\beta_{2}=0.9,0.95$  |\n|  base learning rate | $1 \\mathrm{e}-4$  |\n|  minimum laerning rate | $5 \\mathrm{e}-7$  |\n|  learning rate schedule | cosine decay  |\n|  weight decay | 0.05  |\n\n## C. What artifacts does IML-ViT capture?\n\nTo investigate whether IML-ViT focuses on subtle artifacts as expected, we employ GradCAM [31] to visualize the regions and content the model focuses on, as shown in Figure 12. Additional results are in the Appendix E.4. We can observe that IML-ViT captures the traces around the manipulated region with the help of edge loss. Further, we can observe some extra subtle attention out of the manipulated region in the fourth image, proving the global dependent ability of ViT can help the model trace the tampered region.\n\n![img-11.jpeg](img-11.jpeg)\n\nFigure 12. GradCAM visualization of IML-ViT.\n\n## D. Additional Experiments Results\n\nSince the space limit, we place a part of our results in Protocol No. 1 here.\n\nObjectFormer [34] and CFL-Net [28] evaluate their models fine-tuning with CASIAv2 on AUC. Although this metric may overestimate the models, IML-ViT has still surpassed them, as shown in Table 15.\n\n[Figure 13] Figure 12. GradCAM visualization of IML-ViT.  This figure is an output example, showcasing the results of a machine learning model applied to different images. The figure consists of four panels, each containing an image, a segmentation mask, a heat map, and a bounding box. The images include flowers, a caution wet floor sign, a snowy landscape, and a car. The segmentation masks are white shapes over the images, indicating the model's predictions. The heat maps display the model's confidence levels, with warmer colors indicating higher confidence. The bounding boxes highlight the model's predicted regions. The figure demonstrates the model's ability to identify and segment objects in various scenes, with the heat maps providing insight into the model's decision-making process.\n",
      "has_image": false
    },
    {
      "page_number": 13,
      "markdown": "| Method | CASIAv1 | Coverage | Columbia | NIST16 | MEAN |\n| --- | --- | --- | --- | --- | --- |\n| ObjectFormer [34] | 0.882 | - | - | - | - |\n| CPL-Net [26] | 0.863 | - | - | 0.799 | - |\n| IML-ViT(Ours) | 0.931 | 0.918 | 0.962 | 0.818 | 0.917 |\n\nTable 15: Comparison of AUC scores trained on CASIAv2.\n![img-12.jpeg](img-12.jpeg)\n\nFigure 13: Testing results on Protocol No.1 of IML-ViT compare to ManTra-Net and MVSS-Net. Each dataset has its preference for manipulation types.\n\n## E. Extra Visualization\n\n## E.1. Visualization of Protocol No.1 on other datasets\n\nHere we also present some of the predict masks under Protocol No.1, which was from dataset with other preference on manipulation types. Extended from CASIAv1 and COVERAGE datasets in the main paper, we present results in NIST16 and Columbia datasets here in Figure 13.\n\n## E.2. Qualitative results for ablation study\n\nThe ablation study from Figure 7 evaluates the impact of various components on IML-ViT’s performance: 1) w/o multi-scale: Significant degradation with poor feature detection and blurred outputs. 2) w/o MAE: Improved over the absence of multi-scale, but still blurry with weak edge definition. 3) w/o high-resolution: Noticeable drop in detail and precision, with coarse boundaries. 4) w/o Edge Loss: Less defined edges, preserving overall shape but losing structural details. 5) Full Setup: Produces the most accurate and detailed segmentation maps, capturing fine details and clear boundaries. In summary, the ablation study highlights the critical contributions of each component to the overall performance of IML-ViT. The multi-scale processing, MAE pre-training, high-resolution input, and edge loss each play a vital role in enhancing the model’s ability to produce a high-quality segmentation map.\n\n## E.3. Extra results for CASIA datasets.\n\nTo provide a detailed showcase of IML-ViT’s performance on image manipulation localization tasks, we present additional image results on the CASIA dataset in Figure 14.\n\n## E.4. Extra GradCAM results\n\nHere we provide extra GradCAM results to verify if IMLViT focuses on the artifacts we want it to trace. Artifacts are mainly distributed around the manipulated region with rapid changes. Figure 15 vividly shows that the IML-ViT can effectively discover the artifacts from the image and support its decision.\n\n## E.5. Feature maps between each module\n\nTo gain a deeper understanding of IML-ViT, we present visualizations of feature maps between layers by calculating the average channel dimensions of the feature map. The outcomes are displayed in Figure 16. This visualization process allows us to shed light on the model’s functioning and provides valuable insights into its mechanisms.\n\n## F. Limitation\n\nWe observe a rapid decline in IML-ViT’s performance on the Gaussian blur attack when the filter kernel size exceeded 11, We argue that this is mainly because our motivation is to make the model focus on detailed artifacts, but excessive Gaussian blurring can significantly remove these details, leading to a sudden drop in performance. However, from another perspective, this can actually prove that our model is able to effectively capture artifacts in tampering. Currently, the training does not specifically enhance blur, so we believe that adding enough blur data augmentation can compensate for this issue.\n\n[Figure 14] (a) NIST16, some inpainting  This figure is an output example, likely from a machine learning or computer vision model. It displays various images alongside their corresponding binary masks or segmentation results. The images include a forest scene, a dandelion, a yellow ball on a blue surface, and a wooden floor. Each image is paired with a white silhouette or mask, indicating the model's predictions or segmentations. The black and white areas represent the model's output, with white typically denoting the predicted object or region of interest. The figure demonstrates the model's ability to segment different objects and scenes, with varying levels of accuracy. No specific numbers, thresholds, or units are visible, but the contrast between the white silhouettes and the original images highlights the model's performance.\n\n\n[Figure 15] (b) Columbia, all splicing. Figure 13. Testing results on Protocol No.1 of IML-ViT compare\nto ManTra-Net and MVSS-Net. Each dataset has its preference\nfor manipulation types.  This figure is an output example, likely from a machine learning or computer vision task. It shows a series of grayscale images arranged in a grid format. Each image appears to be a segmentation mask, with white regions representing the predicted object and black representing the background. The top left image shows a scene with a rubber duck and a green bag, while the subsequent images seem to be progressively zoomed-in or cropped views of the same or similar objects. The bottom left image includes a potted plant, which is also segmented. The figure demonstrates the model's ability to segment objects in different scenes, with varying levels of detail and accuracy. No specific numbers, thresholds, or units are visible, but the segmentation quality varies across the images.\n",
      "has_image": false
    },
    {
      "page_number": 14,
      "markdown": "![img-13.jpeg](img-13.jpeg)\n\nFigure 14. Additional CASIAv1 results of IML-ViT. Trained on CASIAv2.\n\n[Figure 16] Figure 14. Additional CASIAv1 results of IML-ViT. Trained on CASIAv2.  This figure is an output example of a machine learning model, specifically for image segmentation. It shows three columns for each image: the original image, the ground truth segmentation, and the model's prediction. The images vary in complexity, from simple shapes to detailed landscapes. The ground truth and prediction are binary masks, with white representing the object of interest and black the background. The figure demonstrates the model's ability to segment various objects within complex scenes, with some images showing accurate predictions and others where the model struggles, particularly with finer details. No specific numbers or thresholds are visible, but the comparison between the ground truth and the model's prediction highlights the model's performance across different scenarios.\n",
      "has_image": false
    },
    {
      "page_number": 15,
      "markdown": "![img-14.jpeg](img-14.jpeg)\n\nFigure 15. Additional GradCAM results. Datasets are collected from CASIAv1, NIST16, Coverage and Columbia. Attention around the manipulated region and long-range dependency could be observed, which is in line with our motivation to force the model to capture the artifacts and compare the relationships between regions explicitly.\n\n[Figure 17] Figure 15. Additional GradCAM results. Datasets are collected from CASIAv1, NIST16, Coverage and Columbia. Attention around the\nmanipulated region and long-range dependency could be observed, which is in line with our motivation to force the model to capture the\nartifacts and compare the relationships between regions explicitly.  This figure is an output example of a machine learning model, specifically for image segmentation. It displays four columns: Image, GroundTruth, GradCAM, and Predict. Each row shows a different image, with corresponding ground truth masks, GradCAM heatmaps, and predicted masks. The GroundTruth column shows the actual segmentation masks, the Predict column shows the model's predictions, and the GradCAM column highlights the areas the model focuses on during prediction. The figure demonstrates the model's performance across various image types, including landscapes, architectural structures, and objects. The GradCAM heatmaps provide insight into the model's decision-making process, showing which parts of the image are most influential in the prediction.\n",
      "has_image": false
    },
    {
      "page_number": 16,
      "markdown": "![img-15.jpeg](img-15.jpeg)\n\nFigure 16. Visualization of the outputs from each component. GT denotes ground truth; Augmented refers to the image after padding and normalize; ViT output is the feature map for the output of ViT backbone; $S-F P N$ denotes the respective output of each resolution for different outputs in simple feature pyramid. For the output of ViT, since visualization takes the average of all channels, we cannot effectively observe the discrepancies between the manipulated region and the authentic region. However, we are pleased to see different types of feature expressions in the multi-level output of $S-F P N$. In the high-resolution output, more representation is given to larger, region-level \"contrast differences\", while in the $(64 \\times 64)$ feature map, we see the image focusing more on edge details and artifacts. This result is in line with the design logic of IML-ViT, which tracks tampering detection from both the perspective of comparing regional low-level differences and capturing detailed visible traces, proving the rationality and effectiveness of our IML-ViT.\n\n[Figure 18] Figure 16. Visualization of the outputs from each component. GT denotes ground truth; Augmented refers to the image after padding\nand normalize; ViT output is the feature map for the output of ViT backbone; S-FPN denotes the respective output of each resolution for\ndifferent outputs in simple feature pyramid. For the output of ViT, since visualization takes the average of all channels, we cannot effectively\nobserve the discrepancies between the manipulated region and the authentic region. However, we are pleased to see different types of feature\nexpressions in the multi-level output of S-FPN. In the high-resolution output, more representation is given to larger, region-level “contrast\ndifferences”, while in the (64×64) feature map, we see the image focusing more on edge details and artifacts. This result is in line with\nthe design logic of IML-ViT, which tracks tampering detection from both the perspective of comparing regional low-level differences and\ncapturing detailed visible traces, proving the rationality and effectiveness of our IML-ViT.  This figure is an output example of a multi-scale feature pyramid network (S-FPN) applied to semantic segmentation tasks. It shows the results of the model on various images, comparing the ground truth (GT) segmentation masks with the model's predictions. The figure includes images, their corresponding GT masks, augmented images, the ViT output, and the S-FPN outputs at different scales (256, 128, 64, 32, 16, 16). The S-FPN outputs are visualized as heatmaps, with the final prediction shown on the far right. The figure demonstrates the model's ability to capture features at multiple scales, with the S-FPN outputs showing more detailed segmentation as the scale decreases. The GT masks are used as a reference to evaluate the accuracy of the model's predictions.\n",
      "has_image": false
    }
  ]
}